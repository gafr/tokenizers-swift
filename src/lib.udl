namespace tokenizers {
  // UniFFI doesn't support associated function, so we have to define them as
  // top-level functions.
  [Throws=TokenizersError]
  RustBpeReadFileReturn models_bpe_bpe_read_file([ByRef] string vocab, [ByRef] string merges);
};

[Error]
enum TokenizersError {
  "Tokenizer",
  "ValueError",
  "Exception",
};

interface RustTokenizer {
  [Name=from_pretrained, Throws=TokenizersError]
  constructor(
    [ByRef] string identifier,
    string revision,
    string? auth_token);

  [Throws=TokenizersError]
  RustEncoding encode([ByRef] string input, boolean add_special_tokens);
};

interface RustEncoding {
  sequence<string> get_tokens();
};

interface RustAddedToken {
  constructor(
    [ByRef] string content,
    boolean? single_word,
    boolean? lstrip,
    boolean? rstrip,
    boolean? normalized,
    boolean? special);

  string get_content();
  boolean get_lstrip();
  boolean get_rstrip();
  boolean get_normalized();
  boolean get_special();
};

// Models
[Custom]
typedef sequence<sequence<string>> RustMerges;

dictionary RustBpeReadFileReturn {
  record<string, u32> vocab;
  RustMerges merges;
};

interface RustBpe {
  [Throws=TokenizersError]
  constructor(
    record<string, u32>? vocab,
    RustMerges? merges,
    string? vocab_file,
    string? merges_file,
    u64? cache_capacity,
    float? dropout,
    string? unk_token,
    string? continuing_subword_prefix,
    string? end_of_word_suffix,
    boolean? fuse_unk
  );

  string? get_unk_token();
};

// Pre-Tokenizers
interface RustPreTokenizedString {
  constructor([ByRef] string content);
};

interface RustWhitespace {
  constructor();
};

// Trainers
interface RustBpeTrainer {
  [Throws=TokenizersError]
  constructor(
    u64? vocab_size,
    u32? min_frequency,
    boolean? show_progress,
    sequence<RustAddedToken>? special_tokens,
    u64? limit_alphabet,
    sequence<string>? initial_alphabet,
    string? continuing_subword_prefix,
    string? end_of_word_suffix
  );

  sequence<RustAddedToken> get_special_tokens();
};